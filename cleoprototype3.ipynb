{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12953869,"sourceType":"datasetVersion","datasetId":8198146},{"sourceId":614273,"sourceType":"modelInstanceVersion","modelInstanceId":425042,"modelId":442523}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nBuilding on from prototype 2. I introduce a new mechanism with the aim of mimicking the brain.\nConsisting of creating a 2nd internal embedding for context embedding representations.\nYou need to train a model from prototype 1 to 40 - 50% accuracy before being introduced here.\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Standard Python Libraries ---\nimport os\nimport re\nimport math\nimport json\nimport unicodedata\nimport random\nimport itertools\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Tuple\n\n# --- Core PyTorch Libraries ---\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkp\nfrom torch import Tensor\nfrom datasets import load_dataset\nfrom torch.utils.data import Sampler\nfrom torch.utils.data import Dataset\nfrom torch.distributions import Categorical\nfrom torch.utils.data import IterableDataset\n\n# --- Hugging Face Libraries ---\n# Required for the tokenizer and learning rate scheduler.\n# pip install transformers\n# pip install tokenizers\nfrom tokenizers import Tokenizer\nfrom transformers import get_linear_schedule_with_warmup\n\n\ndef process_and_extract_qa(raw_example: dict):\n    \"\"\"\n    A generator that takes a raw data dictionary from the stream,\n    extracts all Q&A pairs, and yields them.\n\n    Args:\n        raw_example: A dictionary, e.g., {'text': 'Context... Question: Q1 Answer: A1 Question: Q2...'}\n    \n    Yields:\n        A tuple of (context, question, answer).\n    \"\"\"\n    # The full text is the context for all subsequent questions.\n    full_text = raw_example.get('text', '')\n    if not full_text or 'Question:' not in full_text:\n        return # Skip malformed or empty rows\n\n    # Split the text at the first \"Question:\" to separate the context from the Q&A block.\n    parts = full_text.split('Question:', 1)\n    context = parts[0].strip()\n    qa_block = \"Question:\" + parts[1] # Re-add the first marker for consistent splitting\n\n    # Split the rest of the block by \"Question:\" to isolate each Q&A pair.\n    # The filter(None, ...) removes any empty strings that might result from the split.\n    qa_pairs = filter(None, qa_block.split('Question:'))\n\n    for pair_text in qa_pairs:\n        # For each pair, split by \"Answer:\" to separate the question and answer.\n        if 'Answer:' in pair_text:\n            q_part, a_part = pair_text.split('Answer:', 1)\n            question = q_part.strip()\n            answer = a_part.strip()\n            # Yield the clean, processed triplet.\n            yield (context, question, answer)\n\ndef clean_data(sentence: str):\n    \"\"\"\n    Normalizes and cleans a string by converting to lowercase, removing accents,\n    isolating punctuation, and removing non-alphanumeric characters.\n    \"\"\"\n    # Convert to lowercase, strip whitespace, and remove diacritics (accents).\n    ascii_string = ''.join(\n        c for c in unicodedata.normalize('NFD', sentence.lower().strip())\n        if unicodedata.category(c) != 'Mn'\n    )\n    # Add a space before punctuation to treat it as a separate token.\n    ascii_string = re.sub(r\"([.!?])\", r\" \\1\", ascii_string)\n    # Remove any characters that are not letters or the specified punctuation.\n    ascii_string = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", ascii_string)\n    # A final check to remove any remaining non-ASCII characters.\n    ascii_string = re.sub(r\"[^\\x00-\\x7F]\", r\"\", ascii_string)\n    # Replace multiple whitespace characters with a single space.\n    return re.sub(r\"\\s+\", r\" \", ascii_string).strip()\n\ndef load_and_prepare_data(path, token_path):\n    \"\"\"Loads a JSON file, cleans the text, and prepares the tokenizer.\"\"\"\n    print(\"Loading and cleaning data...\")\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    tokenizer = Tokenizer.from_file(token_path)\n    vocabulary = tokenizer.get_vocab()\n    \n    # Logging the vocabulary size for verification.\n    print(f\"Vocabulary size: {len(vocabulary)}\")\n\n    pairs = []\n    for entry in data:\n        # Assuming the data has been pre-cleaned.\n        pairs.append([entry['input'], entry['target']])\n        \n    return pairs, tokenizer, len(vocabulary)\n\ndef indexesFromSentence(tokenizer, sentence, SOS_token, EOS_token):\n    \"\"\"\n    Encodes a sentence into a list of token indices, adding Start-of-Sentence\n    and End-of-Sentence tokens.\n    \"\"\"\n    encoded_ids = tokenizer.encode(sentence).ids\n    return [SOS_token] + encoded_ids + [EOS_token]\n\ndef pad_tensor(x, PAD_token):\n    \"\"\"Pads a list of tensors to the same length using the PAD_token.\"\"\"\n    padded_tensor = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=PAD_token)\n    return padded_tensor\n\ndef mask_tensor(x, PAD_token):\n    \"\"\"Creates a boolean mask for a tensor, identifying non-PAD tokens.\"\"\"\n    masked_tensor = (x != PAD_token)\n    return masked_tensor\n\ndef batch_to_tensors(tokenizer, batch_pairs, SOS, EOS, PAD):\n    \"\"\"\n    Converts a batch of string pairs into padded and masked tensors\n    for model input.\n    \"\"\"\n    # Unzip the batch of pairs into separate lists for questions and responses.\n    questions, responses = zip(*batch_pairs)\n    \n    # Convert sentences to token indices.\n    questions_indexed = [indexesFromSentence(tokenizer, sentence, SOS, EOS) for sentence in questions]\n    responses_indexed = [indexesFromSentence(tokenizer, sentence, SOS, EOS) for sentence in responses]\n    \n    # Convert lists of indices to PyTorch tensors.\n    question_tensors = [torch.LongTensor(q) for q in questions_indexed]\n    response_tensors = [torch.LongTensor(r) for r in responses_indexed]\n    \n    # Record the original lengths for potential use later (e.g., with RNNs).\n    question_lengths = torch.tensor([len(q) for q in question_tensors], dtype=torch.long)\n    response_lengths = torch.tensor([len(r) for r in response_tensors], dtype=torch.long)\n\n    # Pad the tensors to ensure they are all the same length.\n    padded_questions = pad_tensor(question_tensors, PAD)\n    padded_responses = pad_tensor(response_tensors, PAD)\n    \n    # Create attention masks to ignore the padded areas.\n    mask_questions = mask_tensor(padded_questions, PAD)\n    mask_responses = mask_tensor(padded_responses, PAD)\n\n    return padded_questions, question_lengths, padded_responses, response_lengths, mask_questions, mask_responses\n\n\nclass FiniteStreamSliceDataset(IterableDataset):\n    def __init__(self, dataset_name: str, slice_start: int, slice_size: int, split: str = 'train', shuffle_buffer_size: int = 10000):\n        \"\"\"\n        An iterable dataset that streams a specific, finite slice from a large\n        Hugging Face dataset. This slice can be iterated over multiple times (for multiple epochs).\n\n        Args:\n            dataset_name: The name of the dataset on Hugging Face Hub.\n            slice_start: The starting row index for the slice.\n            slice_size: The number of rows to include in the slice.\n            split: The dataset split to use.\n            shuffle_buffer_size: The buffer size for shuffling within the slice.\n        \"\"\"\n        super().__init__()\n        self.dataset_name = dataset_name\n        self.split = split\n        self.slice_start = slice_start\n        self.slice_size = slice_size\n        self.shuffle_buffer_size = shuffle_buffer_size\n        self.estimated_qa_per_row = 2.5\n        self.estimated_length = int(self.slice_size * self.estimated_qa_per_row)\n\n    def __iter__(self):\n        # The __iter__ method is called by the DataLoader at the beginning of EACH epoch.\n        # This ensures that for every epoch, we go back to the cloud and re-stream\n        # the exact same slice of data.\n\n        # 1. Load the dataset in streaming mode.\n        hf_stream = load_dataset(self.dataset_name, split=self.split, streaming=True)\n\n        # 2. **THE CORE LOGIC**: Skip to the start of our slice and then take only the amount we want.\n        # This is highly efficient as it doesn't process the skipped rows.\n        data_slice = hf_stream.skip(self.slice_start).take(self.slice_size)\n\n        # 3. Shuffle the slice. This is important so the model sees the data\n        # in a different order each epoch, which improves learning.\n        shuffled_slice = data_slice.shuffle(\n            buffer_size=self.shuffle_buffer_size,\n            seed=random.randint(0, 1000) # New seed each epoch for new shuffle order\n        )\n\n        # 4. Process and yield the Q&A pairs from our curated slice.\n        for raw_example in shuffled_slice:\n            for context, question, answer in process_and_extract_qa(raw_example):\n                yield {\"context\": context, \"question\": question, \"answer\": answer}\n    \n    def __len__(self):\n        \"\"\"\n        Returns the estimated number of Q&A pairs in our slice.\n        This allows len(data_loader) to work for progress tracking.\n        \"\"\"\n        return self.estimated_length\n\nclass ChatDataset(Dataset):\n    def __init__(self, pairs):\n        self.pairs = pairs\n\n    def __len__(self):\n        # Returns the total number of samples in the dataset.\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        # Retrieves a single input-target pair from the dataset.\n        return self.pairs[idx]\n\nclass BucketRandomSampler(Sampler):\n    def __init__(self, data_source, bucket_size, batch_size):\n        \"\"\"\n        A custom sampler that groups sentences of similar lengths into buckets,\n        shuffles the batches within those buckets, and then yields the indices.\n        This helps to minimize padding and improve training efficiency.\n\n        Args:\n            data_source: The dataset to sample from.\n            bucket_size: The number of samples to group into a single bucket.\n            batch_size: The desired batch size.\n        \"\"\"\n        super().__init__(data_source)\n        self.data_source = data_source\n        self.bucket_size = bucket_size\n        self.batch_size = batch_size\n\n        # Pre-calculate the combined length of each sentence pair for sorting.\n        self.lengths = [len(pair[0]) + len(pair[1]) for pair in data_source.pairs]\n\n    def __iter__(self):\n        # Create a list of indices from 0 to the length of the dataset.\n        indices = np.arange(len(self.data_source))\n\n        # --- The Bucketing Logic ---\n        # 1. Group indices into larger \"buckets\".\n        num_buckets = (len(self.data_source) + self.bucket_size - 1) // self.bucket_size\n        \n        all_shuffled_indices = []\n\n        for i in range(num_buckets):\n            # Get the indices for the current bucket.\n            start_idx = i * self.bucket_size\n            end_idx = start_idx + self.bucket_size\n            bucket_indices = indices[start_idx:end_idx]\n\n            # 2. Sort the indices within this bucket by sentence length.\n            #    This places sentences of similar lengths next to each other.\n            bucket_lengths = [self.lengths[idx] for idx in bucket_indices]\n            sorted_pairs = sorted(zip(bucket_indices, bucket_lengths), key=lambda x: x[1])\n            sorted_bucket_indices = [x[0] for x in sorted_pairs]\n            \n            # 3. Create batches from the sorted bucket and then shuffle the batches.\n            #    This maintains randomness at the batch level while keeping lengths similar within a batch.\n            batches_in_bucket = []\n            num_batches = (len(sorted_bucket_indices) + self.batch_size - 1) // self.batch_size\n            for j in range(num_batches):\n                batch_start = j * self.batch_size\n                batch_end = batch_start + self.batch_size\n                batches_in_bucket.append(sorted_bucket_indices[batch_start:batch_end])\n\n            # Shuffle the order of the batches within the bucket.\n            np.random.shuffle(batches_in_bucket)\n            \n            # Add the shuffled batches to the final list of indices to be yielded.\n            for batch in batches_in_bucket:\n                all_shuffled_indices.extend(batch)\n        \n        return iter(all_shuffled_indices)\n\n    def __len__(self):\n        return len(self.data_source)\n\nclass Embedding(nn.Module):\n    \n    def __init__(self, total_number, embedding_dimension):\n        super().__init__()\n        self.embedding = nn.Embedding(num_embeddings=total_number, embedding_dim=embedding_dimension, padding_idx=PAD_token)\n        \n    def forward(self, tensor):\n        tensor_id = self.embedding(tensor)\n        return tensor_id\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, embedding_dim: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n        pe = torch.zeros(max_len, 1, embedding_dim)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Injects positional information into the input embeddings.\n\n        Arguments:\n            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(1)].transpose(0, 1)\n        return self.dropout(x)\n\n# Based on: https://github.com/Hanhpt23/Implement-Self-attention-Pytorch/blob/main/self-attention.py\nclass EncoderAttention(nn.Module):\n    \"\"\"    \n    Implementation of self-attention as described in 'Attention Is All You Need'.\n    \n    Note:\n    A sliding context window has been added to manage memory usage, which\n    can be high with long sequences.\n    \"\"\"\n    \n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.1):\n        '''dim: The embedding dimension of the input tokens.'''\n\n        super().__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divisible by num_heads {num_heads}.\"\n\n        self.context_window = 200\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # Using a single linear layer for Q, K, V is more efficient.\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, responses, padding_mask=None):\n        B, N, C = responses.shape\n\n        # self.qkv(x) results in [B, N, 3*C]\n        # .reshape -> [B, N, 3, h, C/h]\n        # .permute -> [3, B, h, N, C/h]\n        qkv = self.qkv(responses).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each becomes [B, h, N, C/h]\n\n        # Calculate scaled dot-product attention.\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n\n        seq_len = responses.size(1)\n    \n        # Create the sliding window mask.\n        # This masks positions where the distance between query and key is too large.\n        rows = torch.arange(seq_len, device=responses.device).unsqueeze(0)\n        cols = torch.arange(seq_len, device=responses.device).unsqueeze(1)\n        window_size = self.context_window // 2\n        \n        # 'window_mask' is True for positions outside the window, which will be masked.\n        window_mask = (rows - cols).abs() > window_size\n        \n        # Apply the sliding window mask.\n        attn = attn.masked_fill(window_mask.unsqueeze(0).unsqueeze(1), float('-inf'))\n\n        # Apply the padding mask.\n        if padding_mask is not None:\n            mask_reshaped = padding_mask.unsqueeze(1).unsqueeze(1)\n            attn = attn.masked_fill(mask_reshaped == 0, float('-inf'))\n\n        # Normalize with softmax over the key dimension.\n        attn = attn.softmax(dim=-1)\n        attn = torch.nan_to_num(attn)\n        attn = self.attn_drop(attn)\n\n        # Multiply with values and reshape for the final output.\n        # (attn @ v) -> [B, h, N, C/h]\n        # .transpose -> [B, N, h, C/h]\n        # .reshape -> [B, N, C]\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n# Similar to the encoder's attention, but with causal and sliding window masking.\nclass DecoderAttention(nn.Module):\n    \n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.1):\n\n        super().__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divisible by num_heads {num_heads}.\"\n\n        \n        self.dim = dim\n        self.context_window = 200\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # A single linear layer for Q, K, V is more efficient.\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, responses, padding_mask=None):\n        B, N, C = responses.shape\n\n        qkv = self.qkv(responses).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n\n        combined_mask = torch.zeros(N, N, device=q.device).bool()\n\n        # Causal mask to prevent attending to future tokens.\n        look_ahead_mask = torch.triu(torch.ones(N, N, device=responses.device), diagonal=1).bool()\n\n        # Sliding window mask.\n        rows = torch.arange(N, device=responses.device).unsqueeze(0)\n        cols = torch.arange(N, device=responses.device).unsqueeze(1)\n        window_size = self.context_window // 2\n        \n        # 'sliding_window_mask' is True for positions outside the window.\n        sliding_window_mask = (rows - cols).abs() > window_size\n\n        # Combine masks: a position is masked if it's in the future OR outside the window.\n        combined_mask = look_ahead_mask | sliding_window_mask\n            \n        # Apply the combined mask to the attention scores.\n        attn = attn.masked_fill(combined_mask.unsqueeze(0).unsqueeze(1), float('-inf'))\n\n        if padding_mask is not None:\n            mask_reshaped = padding_mask.unsqueeze(1).unsqueeze(1)\n            attn = attn.masked_fill(mask_reshaped == 0, float('-inf'))\n\n        attn = attn.softmax(dim=-1)\n        attn = torch.nan_to_num(attn)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n# Inspired by: https://gist.github.com/wolfecameron/5646b2092d41d6d31ec1abb28b3b930a\nclass CrossAttention(nn.Module):\n\n    def __init__(self, embedding_dim):\n        \"\"\"\n        Arguments:\n        embedding_dim: size of the embedding dimension.\n        \"\"\"\n        super().__init__()\n        self.d = embedding_dim\n        \n        # Linear projection for producing the query matrix.\n        self.w_q = nn.Linear(embedding_dim, embedding_dim, bias=False)\n        \n        # Linear projection for producing key and value matrices.\n        self.w_kv = nn.Linear(embedding_dim, 2*embedding_dim, bias=False)\n\n    def forward(self, x_1, x_2, padding_mask=None):\n        # Compute query, key, and value matrices.\n        q = self.w_q(x_1)\n        k, v = self.w_kv(x_2).split(self.d, dim=2)\n\n        # Compute the attention matrix.\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n\n        if padding_mask is not None:\n            # The mask corresponds to the keys/values (x_2).\n            mask_reshaped = padding_mask.unsqueeze(1).unsqueeze(1) # Reshape for broadcasting: [B, 1, 1, N_keys]\n            att = att.masked_fill(mask_reshaped == 0, float('-inf'))\n    \n        att = F.softmax(att, dim=-1)\n        att = torch.nan_to_num(att)\n    \n        y = att @ v\n        return y\n\nclass DifferentiableExplorerAttention(nn.Module):\n    def __init__(self, dim, num_heads=8, num_explorers=10, qkv_bias=False, proj_drop=0.1):\n        super().__init__()\n        assert dim % num_heads == 0, \"Embedding dim must be divisible by num_heads.\"\n        self.num_heads = num_heads\n        self.num_explorers = num_explorers\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        # 1. Standard QKV projection.\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        # 2. A small network to predict explorer positions.\n        #    It takes the query context and outputs a position for each explorer.\n        self.position_predictor = nn.Sequential(\n            nn.Linear(self.head_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, self.num_explorers) # Output E positions\n        )\n\n    def forward(self, x, padding_mask=None):\n        B, N, C = x.shape\n        H, E = self.num_heads, self.num_explorers\n\n        # --- Step 1: Standard QKV ---\n        qkv = self.qkv(x).reshape(B, N, 3, H, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each is [B, H, N, D]\n\n        # Base attention map shared by all explorers.\n        shared_attn_scores = (q @ k.transpose(-2, -1)) * self.scale # [B, H, N, N]\n\n        # --- Step 2: Predict Explorer Positions ---\n        # Use the mean of the queries as a context vector for position prediction.\n        context_q = q.mean(dim=2) # [B, H, D]\n        \n        # Predict positions, constrained between 0 and N-1.\n        predicted_positions = self.position_predictor(context_q).sigmoid() * (N - 1) # [B, H, E]\n\n        # --- Step 3: Create Soft \"Spotlight\" Masks from Positions ---\n        pos_indices = torch.arange(N, device=x.device, dtype=torch.float32).view(1, 1, 1, N) # [1, 1, 1, N]\n        centers = predicted_positions.unsqueeze(-1) # [B, H, E, 1]\n        \n        # A heuristic for the width of the spotlight.\n        spotlight_width = N / (E * 2) \n        \n        # Calculate Gaussian masks based on the predicted centers.\n        exponent = -((pos_indices - centers) ** 2) / (2 * spotlight_width ** 2)\n        spotlight_masks = torch.exp(exponent) # Shape: [B, H, E, N]\n\n        # --- Step 4: Apply Spotlights and Aggregate ---\n        # Modify the attention scores for each query based on where the explorers are looking.\n        # Reshape for broadcasting:\n        # shared_attn_scores: [B, H, N, N] -> [B, H, 1, N, N]\n        # spotlight_masks:    [B, H, E, N] -> [B, H, E, 1, N]\n        weighted_attn_scores = shared_attn_scores.unsqueeze(2) + spotlight_masks.unsqueeze(3)\n\n        if padding_mask is not None:\n            # The mask needs to be broadcastable to [B, H, E, N, N].\n            mask = padding_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n            weighted_attn_scores = weighted_attn_scores.masked_fill(mask == 0, float('-inf'))\n        \n        # Re-normalize with softmax for each explorer's view.\n        weighted_attn_probs = F.softmax(weighted_attn_scores, dim=-1) # [B, H, E, N, N]\n\n        # Get the output for all explorers and fuse them by averaging.\n        # v needs to be [B, H, 1, N, D] for broadcasting.\n        fused_output = (weighted_attn_probs @ v.unsqueeze(2)).mean(dim=2) # [B, H, N, D]\n\n        # --- Step 5: Final Projection ---\n        output = fused_output.transpose(1, 2).reshape(B, N, C)\n        output = self.proj(output)\n        output = self.proj_drop(output)\n\n        # --- Step 6: Repulsion Loss ---\n        # This encourages explorers to spread out and cover different parts of the sequence.\n        p = predicted_positions.view(B*H, E, 1)\n        p_diff = p.transpose(1, 2) - p # [B*H, E, E]\n        \n        # Use a Gaussian kernel for repulsion.\n        repulsion = torch.exp(-(p_diff ** 2) / (2 * spotlight_width ** 2))\n        \n        # Sum over pairs of different explorers, removing the diagonal (self-repulsion).\n        repulsion_loss = repulsion.sum(dim=(-1, -2)) - E\n        repulsion_loss = repulsion_loss.mean()\n\n        # The module returns the final output and the auxiliary loss term.\n        return output, repulsion_loss\n\nclass LayerNormalization(nn.Module):\n    \"\"\"\n    Implements Layer Normalization.\n\n    Normalizes the inputs across the features for each data sample,\n    making the computation independent of batch size.\n\n    y = (x - E[x]) / sqrt(Var[x] + eps) * gamma + beta\n    \"\"\"\n    def __init__(self, d_model: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        # 'gamma' is a learnable scale parameter.\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        # 'beta' is a learnable shift parameter.\n        self.beta = nn.Parameter(torch.zeros(d_model))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # --- The Core Logic of Layer Normalization ---\n\n        # 1. Calculate the mean and variance across the feature dimension.\n        #    'keepdim=True' is important for broadcasting.\n        mean = x.mean(dim=-1, keepdim=True)\n        # Using population variance, which is common in ML.\n        var = x.var(dim=-1, unbiased=False, keepdim=True)\n\n        # 2. Normalize the input tensor.\n        normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n\n        # 3. Scale and shift with the learnable parameters.\n        output = self.gamma * normalized_x + self.beta\n\n        return output\n\nclass FeedForward(nn.Module):\n    \"\"\"\n    The Position-wise Feed-Forward Network from 'Attention Is All You Need'.\n\n    Note:\n    Typically, d_ff is 4 times d_model, but I'm using a smaller size\n    to reduce computational intensity.\n    \"\"\"\n    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # (batch_size, sequence_length, d_model) -> (batch_size, sequence_length, d_ff)\n        x = self.linear_1(x)\n        x = self.relu(x)\n\n        # Applying dropout for regularization.\n        x = self.dropout(x)\n\n        # (batch_size, sequence_length, d_ff) -> (batch_size, sequence_length, d_model)\n        x = self.linear_2(x)\n\n        return x\n\n# In the EncoderLayer class\n\nclass EncoderLayer(nn.Module):\n    \"\"\"    \n    A single layer of the Encoder, based on 'Attention Is All You Need'.\n    \n    Note:\n    The normalization layer is placed before the attention and residual connections\n    (pre-norm) to help with gradient stability during training.\n    The commented-out prints are useful for debugging gradient issues.\n    \"\"\"\n    def __init__(self, embedding_dim):\n        super().__init__()\n        # Using the custom DifferentiableExplorerAttention.\n        self.input_attention = DifferentiableExplorerAttention(dim=embedding_dim, num_heads=8, num_explorers=10)\n        self.normalization1 = LayerNormalization(embedding_dim, 1e-6)\n        self.normalization2 = LayerNormalization(embedding_dim, 1e-6)\n        self.feed = FeedForward(embedding_dim, embedding_dim, 0.1)\n\n    def forward(self, pos_id_tensor, mask):\n        #print(f\"part_1_encoder_input (pos_id_tensor): shape={pos_id_tensor.shape}, mean={pos_id_tensor.mean():.4f}, std={pos_id_tensor.std():.4f}, has_inf={torch.isinf(pos_id_tensor).any()}, has_nan={torch.isnan(pos_id_tensor).any()}\")\n        normal_tensor = self.normalization1(pos_id_tensor)\n        #print(f\"part_2_encoder_norm_1: shape={normal_tensor.shape}, mean={normal_tensor.mean():.4f}, std={normal_tensor.std():.4f}, has_inf={torch.isinf(normal_tensor).any()}, has_nan={torch.isnan(normal_tensor).any()}\")\n        \n        # The attention module returns the output and the repulsion loss.\n        revised_tensor, repulsion_loss = checkp.checkpoint(self.input_attention, normal_tensor, mask, use_reentrant=True) \n        \n        #print(f\"part_3_encoder_input_attention: shape={revised_tensor.shape}, mean={revised_tensor.mean():.4f}, std={revised_tensor.std():.4f}, has_inf={torch.isinf(revised_tensor).any()}, has_nan={torch.isnan(revised_tensor).any()}\")\n        wise_tensor = pos_id_tensor + revised_tensor\n        #print(f\"part_4_encoder_add_1: shape={wise_tensor.shape}, mean={wise_tensor.mean():.4f}, std={wise_tensor.std():.4f}, has_inf={torch.isinf(wise_tensor).any()}, has_nan={torch.isnan(wise_tensor).any()}\")\n        \n        normal_tensor2 = self.normalization2(wise_tensor)\n        #print(f\"part_7_encoder_norm_2 (mistakenly labeled 7): shape={normal_tensor2.shape}, mean={normal_tensor2.mean():.4f}, std={normal_tensor2.std():.4f}, has_inf={torch.isinf(normal_tensor2).any()}, has_nan={torch.isnan(normal_tensor2).any()}\")\n        \n        ffn_output = self.feed(normal_tensor2)\n        #print(f\"part_5_encoder_ffn: shape={ffn_output.shape}, mean={ffn_output.mean():.4f}, std={ffn_output.std():.4f}, has_inf={torch.isinf(ffn_output).any()}, has_nan={torch.isnan(ffn_output).any()}\")\n        \n        wise_tensor2 = wise_tensor + ffn_output\n        #print(f\"part_6_encoder_add_2: shape={wise_tensor2.shape}, mean={wise_tensor2.mean():.4f}, std={wise_tensor2.std():.4f}, has_inf={torch.isinf(wise_tensor2).any()}, has_nan={torch.isnan(wise_tensor2).any()}\")\n        \n        # Pass the repulsion loss up to be handled by the main Encoder.\n        return wise_tensor2, repulsion_loss\n\nclass Encoder(nn.Module):\n    def __init__(self, embedding_dim, embedding, positional, num_layers):\n        super().__init__()\n        self.embedding = embedding\n        self.pos = positional\n        self.layers = nn.ModuleList([EncoderLayer(embedding_dim) for _ in range(num_layers)])\n        \n    def forward(self, tensor, mask):\n        id_tensor = self.embedding.forward(tensor)\n        pos_id_tensor = self.pos(id_tensor)\n\n        all_repulsion_losses = []\n\n        for layer in self.layers:\n            # Each layer returns the tensor and its repulsion loss.\n            pos_id_tensor, repulsion_loss = layer(pos_id_tensor, mask)\n            all_repulsion_losses.append(repulsion_loss)\n        \n        # Average the repulsion loss across all layers.\n        avg_repulsion_loss = torch.stack(all_repulsion_losses).mean()\n\n        # The encoder returns the final output and the average repulsion loss.\n        return pos_id_tensor, avg_repulsion_loss\n\nclass DecoderLayer(nn.Module):\n    \"\"\"    \n    A single layer of the Decoder, based on 'Attention Is All You Need'.\n    \n    Note:\n    Using pre-norm for gradient stability.\n    The commented-out prints are kept for debugging purposes.\n    \"\"\"\n    def __init__(self, embedding_dim):\n        super().__init__()\n        self.output_attention = DecoderAttention(embedding_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.1)\n        self.cross_attention = CrossAttention(embedding_dim)\n        self.normalization1 = LayerNormalization(embedding_dim, 1e-6)\n        self.normalization2 = LayerNormalization(embedding_dim, 1e-6)\n        self.normalization3 = LayerNormalization(embedding_dim, 1e-6)\n        self.feed = FeedForward(embedding_dim, embedding_dim, 0.1)\n        \n    def forward(self, encoder_tensor, padded_r, mask):\n        #print(f\"part_1_decoder_input (padded_r): shape={padded_r.shape}, mean={padded_r.mean():.4f}, std={padded_r.std():.4f}, has_inf={torch.isinf(padded_r).any()}, has_nan={torch.isnan(padded_r).any()}\")\n        normal_tensor = self.normalization1(padded_r)\n        #print(f\"part_2_decoder_norm_1: shape={normal_tensor.shape}, mean={normal_tensor.mean():.4f}, std={normal_tensor.std():.4f}, has_inf={torch.isinf(normal_tensor).any()}, has_nan={torch.isnan(normal_tensor).any()}\")\n        revised_tensor = checkp.checkpoint(self.output_attention, normal_tensor, mask, use_reentrant=True) # Batch, Sequence, Embedding\n        #print(f\"part_3_decoder_output_attention: shape={revised_tensor.shape}, mean={revised_tensor.mean():.4f}, std={revised_tensor.std():.4f}, has_inf={torch.isinf(revised_tensor).any()}, has_nan={torch.isnan(revised_tensor).any()}\")\n        wise_tensor = padded_r + revised_tensor # Batch, Sequence, Embedding\n        #print(f\"part_4_decoder_add_1: shape={wise_tensor.shape}, mean={wise_tensor.mean():.4f}, std={wise_tensor.std():.4f}, has_inf={torch.isinf(wise_tensor).any()}, has_nan={torch.isnan(wise_tensor).any()}\")\n        \n        normal_tensor2 = self.normalization2(wise_tensor)\n        #print(f\"part_5_decoder_norm_2: shape={normal_tensor2.shape}, mean={normal_tensor2.mean():.4f}, std={normal_tensor2.std():.4f}, has_inf={torch.isinf(normal_tensor2).any()}, has_nan={torch.isnan(normal_tensor2).any()}\")\n        revised_tensor = checkp.checkpoint(self.cross_attention, normal_tensor2, encoder_tensor, use_reentrant=True) # Batch, Sequence, Embedding\n        #print(f\"part_6_decoder_cross_attention: shape={revised_tensor.shape}, mean={revised_tensor.mean():.4f}, std={revised_tensor.std():.4f}, has_inf={torch.isinf(revised_tensor).any()}, has_nan={torch.isnan(revised_tensor).any()}\")\n        wise_tensor2 = wise_tensor + revised_tensor\n        #print(f\"part_7_decoder_add_2: shape={wise_tensor2.shape}, mean={wise_tensor2.mean():.4f}, std={wise_tensor2.std():.4f}, has_inf={torch.isinf(wise_tensor2).any()}, has_nan={torch.isnan(wise_tensor2).any()}\")\n\n        normal_tensor3 = self.normalization3(wise_tensor2)\n        #print(f\"part_8_decoder_output (normal_tensor3): shape={normal_tensor3.shape}, mean={normal_tensor3.mean():.4f}, std={normal_tensor3.std():.4f}, has_inf={torch.isinf(normal_tensor3).any()}, has_nan={torch.isnan(normal_tensor3).any()}\")\n        ffn_output = self.feed(normal_tensor3)\n        #print(f\"part_9_decoder_ffn: shape={ffn_output.shape}, mean={ffn_output.mean():.4f}, std={ffn_output.std():.4f}, has_inf={torch.isinf(ffn_output).any()}, has_nan={torch.isnan(ffn_output).any()}\")\n        wise_tensor3 = wise_tensor2 + ffn_output\n        #print(f\"part_10_decoder_add_3: shape={wise_tensor3.shape}, mean={wise_tensor3.mean():.4f}, std={wise_tensor3.std():.4f}, has_inf={torch.isinf(wise_tensor3).any()}, has_nan={torch.isnan(wise_tensor3).any()}\")\n\n        return wise_tensor3\n\nclass Decoder(nn.Module):\n    \"\"\"    \n    The Decoder module, composed of multiple DecoderLayers.\n    \"\"\"\n    def __init__(self, embedding_dim, embedding, positional, num_layers):\n        super().__init__()\n        self.embedding = embedding\n        self.pos = positional\n        self.layers = nn.ModuleList([DecoderLayer(embedding_dim) for _ in range(num_layers)])\n        \n    def forward(self, encoder_tensor, responses, mask_r):\n        id_tensor = self.embedding.forward(responses)\n        pos_id_tensor = self.pos(id_tensor) #Batch, Sequence, Embedding\n\n        for layer in self.layers:\n            pos_id_tensor = layer(encoder_tensor, pos_id_tensor, mask_r)\n\n        return pos_id_tensor\n\nclass Cleopatra(nn.Module):\n    def __init__(self, decoder, encoder, embedding_dim, total_num, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.ln_0 = nn.Linear(embedding_dim, total_num, bias=False)\n        self.device = device\n        # Hyperparameter for the \"Small-World\" repulsion margin.\n        self.sw_repulsion_margin = 0.5\n\n    def calculate_small_world_repulsion_loss(self, encoder_output):\n        \"\"\"\n        Calculates the \"Small-World\" repulsion loss on the encoder's output embeddings.\n        This encourages distinct concepts within the batch to have separate representations.\n        \"\"\"\n        # 1. Get a single vector representation for each sequence by mean pooling.\n        #    Shape: [batch_size, seq_len, embedding_dim] -> [batch_size, embedding_dim]\n        sentence_embeddings = encoder_output.mean(dim=1)\n        \n        batch_size = sentence_embeddings.size(0)\n        # The loss is only meaningful if there are pairs to compare.\n        if batch_size <= 1:\n            return torch.tensor(0.0, device=encoder_output.device)\n\n        # 2. Efficiently calculate all pairwise L2 distances.\n        pairwise_dist = torch.cdist(sentence_embeddings, sentence_embeddings, p=2)\n        \n        # 3. Identify \"blurry\" concepts by finding the \"hardest negative\".\n        #    To do this, we ignore the distance of each embedding to itself (the diagonal).\n        diagonal_mask = torch.eye(batch_size, device=pairwise_dist.device, dtype=torch.bool)\n        \n        # The hardest negative for each concept is the one closest to it.\n        hardest_negative_dist, _ = torch.min(pairwise_dist, dim=1)\n        \n        # 4. Apply the targeted repulsion loss.\n        #    The loss is max(0, margin - distance), penalizing only pairs that are too close.\n        loss = torch.clamp(self.sw_repulsion_margin - hardest_negative_dist, min=0)\n        \n        # Return the average loss across the batch.\n        return loss.mean()\n    \n    def forward(self, questions, responses, mask_r, mask_q, context=None, context_mask=None):\n        \"\"\"\n        The updated forward pass that now accepts the optional context and its mask.\n        \"\"\"\n        # If context is provided, combine it with the question for the encoder.\n        if context is not None and context_mask is not None:\n            encoder_input = torch.cat([context, questions], dim=1)\n            encoder_mask = torch.cat([context_mask, mask_q], dim=1)\n        # Otherwise, use only the question.\n        else:\n            encoder_input = questions\n            encoder_mask = mask_q\n            \n        # 1. Encoder runs on the correct input\n        enc_output, explorer_repulsion_loss = self.encoder(encoder_input, encoder_mask)\n        \n        # 2. Calculate the small-world repulsion loss\n        sw_repulsion_loss = self.calculate_small_world_repulsion_loss(enc_output)\n        \n        # 3. Decoder runs as before\n        dec_output = self.decoder(enc_output, responses, mask_r)\n        logits = self.ln_0(dec_output)\n        \n        # 4. Combine repulsion losses\n        total_repulsion_loss = explorer_repulsion_loss + sw_repulsion_loss\n        \n        return logits, total_repulsion_loss\n\ndef collate_fn_with_context(batch):\n    \"\"\"\n    Processes a batch of dictionaries and returns all 8 required tensors,\n    including sequence lengths.\n    \"\"\"\n    # Separate the data from the batch of dicts.\n    contexts = [item['context'] for item in batch]\n    questions = [item['question'] for item in batch]\n    answers = [item['answer'] for item in batch]\n\n    # --- Process Context ---\n    context_indexed = [indexesFromSentence(tokenizer, sentence, SOS_token, EOS_token) for sentence in contexts]\n    context_tensors = [torch.LongTensor(c) for c in context_indexed]\n    padded_contexts = pad_tensor(context_tensors, PAD_token)\n    mask_contexts = mask_tensor(padded_contexts, PAD_token)\n\n    # --- Process Questions ---\n    questions_indexed = [indexesFromSentence(tokenizer, sentence, SOS_token, EOS_token) for sentence in questions]\n    question_tensors = [torch.LongTensor(q) for q in questions_indexed]\n    # **NEW**: Calculate question lengths\n    question_lengths = torch.tensor([len(q) for q in question_tensors], dtype=torch.long)\n    padded_questions = pad_tensor(question_tensors, PAD_token)\n    mask_questions = mask_tensor(padded_questions, PAD_token)\n\n    # --- Process Answers ---\n    answers_indexed = [indexesFromSentence(tokenizer, sentence, SOS_token, EOS_token) for sentence in answers]\n    answer_tensors = [torch.LongTensor(a) for a in answers_indexed]\n    # **NEW**: Calculate answer lengths\n    answer_lengths = torch.tensor([len(a) for a in answer_tensors], dtype=torch.long)\n    padded_answers = pad_tensor(answer_tensors, PAD_token)\n    mask_answers = mask_tensor(padded_answers, PAD_token)\n\n    # --- CORRECTED RETURN STATEMENT ---\n    # Now returns all 8 items in the expected order.\n    return (padded_questions, question_lengths, padded_answers, answer_lengths,\n            mask_questions, mask_answers, padded_contexts, mask_contexts)\n\ndef save_checkpoint(directory, filename, model_state, optimizer_state, scheduler_state, scaler_state, stats):\n    \"\"\"Saves a comprehensive training checkpoint.\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    checkpoint_path = os.path.join(directory, filename)\n\n    # Create a single dictionary to hold all necessary information for resuming training.\n    checkpoint = {\n        'model_state_dict': model_state,\n        'optimizer_state_dict': optimizer_state,\n        'scheduler_state_dict': scheduler_state,\n        'scaler_state_dict': scaler_state,\n        # Unpack the dictionary of metrics and other info.\n        **stats \n    }\n    \n    torch.save(checkpoint, checkpoint_path)\n    print(f\"\\n--- Checkpoint Saved ---\")\n    print(f\"Path: {checkpoint_path}\")\n    print(f\"Epoch: {stats.get('epoch', 'N/A')}, Global Step: {stats.get('global_step', 'N/A')}\")\n    print(f\"Metrics: Total Loss={stats['metrics'].get('total_loss', 0):.4f}, Accuracy={stats['metrics'].get('accuracy', 0):.2f}%\")\n    print(f\"------------------------\\n\")\n\ndef training_goal_dual_path(cleopatra, criterion, padded_q, padded_r, mask_q, mask_r,\n                              # New arguments for context\n                              padded_context, mask_context,\n                              device, temperature, p_context=0.5):\n    \"\"\"\n    Performs the dual-path training logic.\n\n    Returns three separate losses: contextual, generalist, and repulsion.\n    \"\"\"\n    decoder_input = padded_r[:, :-1]\n    mask_r_shifted = mask_r[:, :-1]\n    targets = padded_r[:, 1:].to(device)\n\n    # --- Path 1: Generalist Thinker (Always run) ---\n    with torch.amp.autocast('cuda'):\n        # Forward pass without any context\n        gen_logits, gen_repulsion_loss = cleopatra(\n            padded_q, decoder_input, mask_r_shifted, mask_q,\n            context=None, context_mask=None\n        )\n        gen_logits = gen_logits / temperature\n        flat_gen_logits = gen_logits.view(-1, gen_logits.shape[-1])\n        flat_targets = targets.reshape(-1)\n        generalist_loss = criterion(flat_gen_logits, flat_targets)\n\n    # --- Path 2: Contextual Expert (Run intermittently) ---\n    contextual_loss = torch.tensor(0.0, device=device) # Default to zero\n    # Decide whether to run the contextual path for this batch\n    if random.random() < p_context:\n        with torch.amp.autocast('cuda'):\n            # Forward pass WITH the context\n            ctx_logits, ctx_repulsion_loss = cleopatra(\n                padded_q, decoder_input, mask_r_shifted, mask_q,\n                context=padded_context, context_mask=mask_context\n            )\n            ctx_logits = ctx_logits / temperature\n            flat_ctx_logits = ctx_logits.view(-1, ctx_logits.shape[-1])\n            contextual_loss = criterion(flat_ctx_logits, flat_targets)\n    else:\n        # If we don't run the contextual path, there's no repulsion loss from it\n        ctx_repulsion_loss = torch.tensor(0.0, device=device)\n\n    # Average the repulsion losses from both runs (one might be zero)\n    total_repulsion_loss = (gen_repulsion_loss + ctx_repulsion_loss) / (2.0 if 'ctx_logits' in locals() else 1.0)\n\n    # For accuracy calculation, we can use the generalist logits as a consistent measure\n    return contextual_loss, generalist_loss, total_repulsion_loss, flat_gen_logits, flat_targets\n\n# --- Constants ---\nMIN_COUNT = 1\nmax_length = 1000\ndropout = 0.1\nbatch = 10\nembedding_dim = 256\nlayers = 3\nepochs = 20\naccumulation_steps = 8\n\nTEACHER_FORCING_RATIO = 1.0\nTEACHER_FORCING_DECAY = 0.01\ncheckpoint = True\nCHECKPOINT_PATH = \"/kaggle/input/cleoprototype100k/pytorch/default/10/CleoPrototype_E1_S20000.pt\"\n\n\"\"\"\nKey Metrics for Training:\n\nLoss: The primary measure of how well the model is performing. A lower score is better.\n\nPerplexity: Measures how \"surprised\" the model is by the next token. It's derived from the cross-entropy loss. Lower is better.\n\nAccuracy: The percentage of times the model's top prediction is correct. More intuitive than loss.\n\nGradNorm: The overall size of the gradients. Helps diagnose exploding gradients (if the norm is very large) or vanishing gradients (if the norm is close to zero).\n\nLearningRate: Tracking the learning rate ensures the scheduler is working as intended.\n\"\"\"\n\ndef main_worker(epochs, data_loader, SOS_token, EOS_token, PAD_token, checkpoint, num_warmup_steps, num_training_steps):\n    embedding = Embedding(total_tokens, embedding_dim)\n    pos_encoding = PositionalEncoding(embedding_dim, dropout)\n    encoder = Encoder(embedding_dim, embedding, pos_encoding, layers)\n    decoder = Decoder(embedding_dim, embedding, pos_encoding, layers)\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    cleopatra = Cleopatra(decoder, encoder, embedding_dim, total_tokens, device)\n    cleopatra.ln_0.weight = embedding.embedding.weight\n\n    if checkpoint:\n        print(\"Loading checkpoint...\")\n        checkpoint_data = torch.load(CHECKPOINT_PATH, weights_only=False, map_location=device)\n        \n        # --- Load Model State ---\n        # Handle models saved with DataParallel, which adds a 'module.' prefix.\n        model_state_dict = checkpoint_data['model_state_dict']\n        if any(key.startswith('module.') for key in model_state_dict):\n            print(\"Model was trained with DataParallel. Removing 'module.' prefix from keys.\")\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in model_state_dict.items():\n                name = k[7:] # remove `module.`\n                new_state_dict[name] = v\n            model_state_dict = new_state_dict\n            \n        cleopatra.load_state_dict(model_state_dict)\n        print(\"Model loaded successfully!\")\n\n    cleopatra.to(device)\n\n    if torch.cuda.device_count() > 1:\n        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n        cleopatra = nn.DataParallel(cleopatra)\n\n    cleopatra = torch.compile(cleopatra)\n    \n    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n    optimizer = torch.optim.AdamW(cleopatra.parameters(), lr=0.0001)\n    \n    if checkpoint:\n        optimizer.load_state_dict(checkpoint_data['optimizer_state_dict'])\n    \n    scheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=num_training_steps\n    )\n    \n    print(f\"Total training steps: {num_training_steps}\")\n    print(f\"Warmup steps: {num_warmup_steps}\")\n\n    scaler = torch.amp.GradScaler('cuda')\n\n    INITIAL_TEMP = 4.0\n    FINAL_TEMP = 1.0\n\n    ANNEAL_START_STEP = 5000 \n    ANNEAL_END_STEP = 25000 \n\n    print(\"\\nStarting training...\")\n    for epoch in range(epochs):\n        for i, (padded_q, len_q, padded_r, len_r, mask_q, mask_r, padded_context, mask_context) in enumerate(data_loader):\n            \n            # --- Calculate the current temperature for this step ---\n            optimizer_step = (i + 1) // accumulation_steps # The actual step number.\n            if optimizer_step < ANNEAL_START_STEP:\n                current_temp = INITIAL_TEMP\n            elif optimizer_step >= ANNEAL_END_STEP:\n                current_temp = FINAL_TEMP\n            else:\n                # Linearly anneal the temperature.\n                progress = (optimizer_step - ANNEAL_START_STEP) / (ANNEAL_END_STEP - ANNEAL_START_STEP)\n                current_temp = INITIAL_TEMP - progress * (INITIAL_TEMP - FINAL_TEMP)\n            \n            # --- Call the new dual-path function ---\n            contextual_loss, generalist_loss, repulsion_loss, flat_logits, flat_targets = training_goal_dual_path(\n                cleopatra, criterion,\n                padded_q, padded_r, mask_q, mask_r,\n                padded_context, mask_context,  # Pass the new context tensors\n                device, current_temp, p_context=0.7 # Using context 70% of the time\n            )\n\n            # --- Combine the losses with weights ---\n            # These weights are key hyperparameters.\n            # w_ctx=0.6: We care a lot about being factually correct when given context.\n            # w_gen=0.4: We also want it to be a good general conversationalist.\n            # w_rep=0.01: Repulsion is an auxiliary goal, keep its weight low.\n            contextual_weight = 0.6\n            generalist_weight = 0.4\n            repulsion_weight = 0.01\n        \n            # The final loss is a weighted sum of the components.\n            # Note: We only add the contextual_loss if it was actually computed (is > 0).\n            loss_components = [generalist_weight * generalist_loss, repulsion_weight * repulsion_loss]\n            if contextual_loss > 0:\n                loss_components.append(contextual_weight * contextual_loss)\n        \n            total_loss = sum(loss_components)\n\n            total_loss = total_loss / accumulation_steps\n            scaler.scale(total_loss).backward()\n        \n            # --- Backpropagation ---\n            if (i + 1) % accumulation_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                scheduler.step()\n        \n            # --- Logging ---\n            if i % 1 == 0:\n                \n                perplexity = torch.exp(generalist_loss).item()\n                preds = torch.argmax(flat_logits.detach(), dim=1)\n                non_pad = (flat_targets != PAD_token)\n                correct = (preds[non_pad] == flat_targets[non_pad]).sum().item()\n                total_non_pad = non_pad.sum().item()\n                accuracy = (correct / total_non_pad) * 100 if total_non_pad > 0 else 0.0\n                learning_rate = optimizer.param_groups[0]['lr']\n\n                # --- Analytical Logging ---\n                print(f\"Epoch {epoch+1} - Step {(i + 1) // accumulation_steps}/{len(data_loader) // accumulation_steps}\")\n                print(f\"  - Total Loss    : {total_loss.item():.4f}\")\n                print(f\"  - Contextual Loss: {contextual_loss.item():.4f}\") # Log this to see how it learns with context\n                print(f\"  - Main Loss     : {generalist_loss.item():.4f}\")\n                print(f\"  - Repulsion Loss: {repulsion_loss.item():.4f}\") \n                print(f\"  - Perplexity (PPL): {perplexity:.2f}\")\n                print(f\"  - Accuracy      : {accuracy:.2f}%\")\n                print(f\"  - Learning Rate : {learning_rate:.9f}\")\n                print(\"-\" * 20)\n\n                del preds, non_pad\n\n            if i != 0 and i % 20000 == 0:\n                global_step = (i + 1) // accumulation_steps\n                stats = {\n                    'epoch': epoch + 1,\n                    'global_step': global_step,\n                    'tokenizer': tokenizer, # Saving the tokenizer is good practice.\n                    'metrics': {\n                        'total_loss': total_loss.item() * accumulation_steps, # Un-normalize for logging.\n                        'main_loss': main_loss.item(),\n                        'repulsion_loss': repulsion_loss.item(),\n                        'perplexity': perplexity,\n                        'accuracy': accuracy,\n                        'learning_rate': learning_rate,\n                        'temperature': current_temp\n                    }\n                }\n\n                # Call the save checkpoint function.\n                save_checkpoint(\n                    directory=\"/kaggle/working/\", \n                    filename=f\"CleoPrototype_E{epoch+1}_S{global_step}.pt\", # A more descriptive filename.\n                    model_state=cleopatra.module.state_dict(), \n                    optimizer_state=optimizer.state_dict(), \n                    scheduler_state=scheduler.state_dict(),\n                    scaler_state=scaler.state_dict(),\n                    stats=stats\n                )\n        \n            del padded_q, padded_r, len_q, len_r, mask_q, mask_r, flat_logits, flat_targets\n            torch.cuda.empty_cache()\n\n\n# --- Main Execution ---\nif __name__ == '__main__':\n    \n    # --- 1. Load data and initialize tokenizer ---\n    token_path = \"/kaggle/input/sub-tokens/my_tokenizer.json\"\n    DATASET_NAME = \"fineinstructions-pretraining/nemotron_qa_1T\"\n    \n    START_ROW = 0\n    NUM_ROWS_TO_TRAIN_ON = 5000\n\n    print(f\"--- Training Curriculum ---\")\n    print(f\"Dataset Slice: Rows {START_ROW} to {START_ROW + NUM_ROWS_TO_TRAIN_ON}\")\n    print(f\"---------------------------\")\n    \n    # Initialize vocabulary.\n    print(\"Building vocabulary...\")\n    #pairs, tokenizer, total_tokens = load_and_prepare_data(file_path, token_path)\n\n    tokenizer = Tokenizer.from_file(token_path)\n    vocabulary = tokenizer.get_vocab()\n    total_tokens = len(vocabulary)\n\n    SOS_token = tokenizer.token_to_id(\"[SOS]\")\n    EOS_token = tokenizer.token_to_id(\"[EOS]\")\n    PAD_token = tokenizer.token_to_id(\"[PAD]\")\n\n    #chat_dataset = ChatDataset(pairs)\n\n    # 1. Instantiate the new \"slice\" dataset\n    finite_dataset = FiniteStreamSliceDataset(\n        dataset_name=\"fineinstructions-pretraining/nemotron_qa_1T\",\n        slice_start=START_ROW,\n        slice_size=NUM_ROWS_TO_TRAIN_ON,\n        shuffle_buffer_size=10000 # Shuffle buffer can be the size of the slice\n    )\n    \n    # 2. Create the DataLoader to build the pipeline\n    # THIS IS THE KEY TO PARALLELISM\n    data_loader = torch.utils.data.DataLoader(\n        dataset=finite_dataset,\n        batch_size=batch,\n        num_workers=1, # Use 2 or 4 cores on Kaggle\n        collate_fn=collate_fn_with_context, # Your collate function from before\n        prefetch_factor=2\n    )\n\n    estimated_items_per_epoch = NUM_ROWS_TO_TRAIN_ON * 2.5 # A reasonable guess\n    steps_per_epoch = estimated_items_per_epoch / batch\n    num_training_steps = epochs * steps_per_epoch\n    num_warmup_steps = 4000 # Can remain fixed or be a percentage of total steps\n\n    # 3. Pass this data_loader to your main_worker.\n    # Your training loop does not need to change at all. It will simply run\n    # for `epochs` number of times over this curated data slice.\n    main_worker(epochs, data_loader, SOS_token, EOS_token, PAD_token, checkpoint, num_warmup_steps, num_training_steps)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}