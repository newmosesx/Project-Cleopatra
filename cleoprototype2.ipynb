{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12942568,"sourceType":"datasetVersion","datasetId":8190381},{"sourceId":12943175,"sourceType":"datasetVersion","datasetId":8190792},{"sourceId":12953869,"sourceType":"datasetVersion","datasetId":8198146},{"sourceId":545198,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":418672,"modelId":436323},{"sourceId":614273,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":425042,"modelId":442523}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nI introduced another learning variable in repulsion loss. Meant to unblurr concepts.\nMeaning concepts that are way too similar should repel each other.\nStill on its testing phase.\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Standard Python Libraries ---\nimport os\nimport re\nimport math\nimport json\nimport unicodedata\nimport random\nimport itertools\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Tuple\n\n# --- Core PyTorch Libraries ---\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\nfrom torch import Tensor\nimport numpy as np\nfrom torch.utils.data import Sampler\nfrom torch.utils.data import Dataset\nimport torch.utils.checkpoint as checkp\n\n# --- Hugging Face Libraries ---\n# Required for the tokenizer and learning rate scheduler.\n# pip install transformers\n# pip install tokenizers\nfrom tokenizers import Tokenizer\nfrom transformers import get_linear_schedule_with_warmup","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_data(sentence: str):\n    \"\"\"\n    Normalizes and cleans a string by converting to lowercase, removing accents,\n    isolating punctuation, and removing non-alphanumeric characters.\n    \"\"\"\n    # Convert to lowercase, strip whitespace, and remove diacritics (accents).\n    ascii_string = ''.join(\n        c for c in unicodedata.normalize('NFD', sentence.lower().strip())\n        if unicodedata.category(c) != 'Mn'\n    )\n    # Add a space before punctuation to treat it as a separate token.\n    ascii_string = re.sub(r\"([.!?])\", r\" \\1\", ascii_string)\n    # Remove any characters that are not letters or the specified punctuation.\n    ascii_string = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", ascii_string)\n    # A final check to remove any remaining non-ASCII characters.\n    ascii_string = re.sub(r\"[^\\x00-\\x7F]\", r\"\", ascii_string)\n    # Replace multiple whitespace characters with a single space.\n    return re.sub(r\"\\s+\", r\" \", ascii_string).strip()\n\ndef load_and_prepare_data(path, token_path):\n    \"\"\"Loads a JSON file, cleans the text, and prepares the tokenizer.\"\"\"\n    print(\"Loading and cleaning data...\")\n    with open(path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n\n    tokenizer = Tokenizer.from_file(token_path)\n    vocabulary = tokenizer.get_vocab()\n    \n    # Logging the vocabulary size for verification.\n    print(f\"Vocabulary size: {len(vocabulary)}\")\n\n    pairs = []\n    for entry in data:\n        # Assuming the data has been pre-cleaned.\n        pairs.append([entry['input'], entry['target']])\n        \n    return pairs, tokenizer, len(vocabulary)\n\ndef indexesFromSentence(tokenizer, sentence, SOS_token, EOS_token):\n    \"\"\"\n    Encodes a sentence into a list of token indices, adding Start-of-Sentence\n    and End-of-Sentence tokens.\n    \"\"\"\n    encoded_ids = tokenizer.encode(sentence).ids\n    return [SOS_token] + encoded_ids + [EOS_token]\n\ndef pad_tensor(x, PAD_token):\n    \"\"\"Pads a list of tensors to the same length using the PAD_token.\"\"\"\n    padded_tensor = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=PAD_token)\n    return padded_tensor\n\ndef mask_tensor(x, PAD_token):\n    \"\"\"Creates a boolean mask for a tensor, identifying non-PAD tokens.\"\"\"\n    masked_tensor = (x != PAD_token)\n    return masked_tensor\n\ndef batch_to_tensors(tokenizer, batch_pairs, SOS, EOS, PAD):\n    \"\"\"\n    Converts a batch of string pairs into padded and masked tensors\n    for model input.\n    \"\"\"\n    # Unzip the batch of pairs into separate lists for questions and responses.\n    questions, responses = zip(*batch_pairs)\n    \n    # Convert sentences to token indices.\n    questions_indexed = [indexesFromSentence(tokenizer, sentence, SOS, EOS) for sentence in questions]\n    responses_indexed = [indexesFromSentence(tokenizer, sentence, SOS, EOS) for sentence in responses]\n    \n    # Convert lists of indices to PyTorch tensors.\n    question_tensors = [torch.LongTensor(q) for q in questions_indexed]\n    response_tensors = [torch.LongTensor(r) for r in responses_indexed]\n    \n    # Record the original lengths for potential use later (e.g., with RNNs).\n    question_lengths = torch.tensor([len(q) for q in question_tensors], dtype=torch.long)\n    response_lengths = torch.tensor([len(r) for r in response_tensors], dtype=torch.long)\n\n    # Pad the tensors to ensure they are all the same length.\n    padded_questions = pad_tensor(question_tensors, PAD)\n    padded_responses = pad_tensor(response_tensors, PAD)\n    \n    # Create attention masks to ignore the padded areas.\n    mask_questions = mask_tensor(padded_questions, PAD)\n    mask_responses = mask_tensor(padded_responses, PAD)\n\n    return padded_questions, question_lengths, padded_responses, response_lengths, mask_questions, mask_responses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ChatDataset(Dataset):\n    def __init__(self, pairs):\n        self.pairs = pairs\n\n    def __len__(self):\n        # Returns the total number of samples in the dataset.\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        # Retrieves a single input-target pair from the dataset.\n        return self.pairs[idx]\n\nclass BucketRandomSampler(Sampler):\n    def __init__(self, data_source, bucket_size, batch_size):\n        \"\"\"\n        A custom sampler that groups sentences of similar lengths into buckets,\n        shuffles the batches within those buckets, and then yields the indices.\n        This helps to minimize padding and improve training efficiency.\n\n        Args:\n            data_source: The dataset to sample from.\n            bucket_size: The number of samples to group into a single bucket.\n            batch_size: The desired batch size.\n        \"\"\"\n        super().__init__(data_source)\n        self.data_source = data_source\n        self.bucket_size = bucket_size\n        self.batch_size = batch_size\n\n        # Pre-calculate the combined length of each sentence pair for sorting.\n        self.lengths = [len(pair[0]) + len(pair[1]) for pair in data_source.pairs]\n\n    def __iter__(self):\n        # Create a list of indices from 0 to the length of the dataset.\n        indices = np.arange(len(self.data_source))\n\n        # --- The Bucketing Logic ---\n        # 1. Group indices into larger \"buckets\".\n        num_buckets = (len(self.data_source) + self.bucket_size - 1) // self.bucket_size\n        \n        all_shuffled_indices = []\n\n        for i in range(num_buckets):\n            # Get the indices for the current bucket.\n            start_idx = i * self.bucket_size\n            end_idx = start_idx + self.bucket_size\n            bucket_indices = indices[start_idx:end_idx]\n\n            # 2. Sort the indices within this bucket by sentence length.\n            #    This places sentences of similar lengths next to each other.\n            bucket_lengths = [self.lengths[idx] for idx in bucket_indices]\n            sorted_pairs = sorted(zip(bucket_indices, bucket_lengths), key=lambda x: x[1])\n            sorted_bucket_indices = [x[0] for x in sorted_pairs]\n            \n            # 3. Create batches from the sorted bucket and then shuffle the batches.\n            #    This maintains randomness at the batch level while keeping lengths similar within a batch.\n            batches_in_bucket = []\n            num_batches = (len(sorted_bucket_indices) + self.batch_size - 1) // self.batch_size\n            for j in range(num_batches):\n                batch_start = j * self.batch_size\n                batch_end = batch_start + self.batch_size\n                batches_in_bucket.append(sorted_bucket_indices[batch_start:batch_end])\n\n            # Shuffle the order of the batches within the bucket.\n            np.random.shuffle(batches_in_bucket)\n            \n            # Add the shuffled batches to the final list of indices to be yielded.\n            for batch in batches_in_bucket:\n                all_shuffled_indices.extend(batch)\n        \n        return iter(all_shuffled_indices)\n\n    def __len__(self):\n        return len(self.data_source)\n\nclass Embedding(nn.Module):\n    \n    def __init__(self, total_number, embedding_dimension):\n        super().__init__()\n        self.embedding = nn.Embedding(num_embeddings=total_number, embedding_dim=embedding_dimension, padding_idx=PAD_token)\n        \n    def forward(self, tensor):\n        tensor_id = self.embedding(tensor)\n        return tensor_id\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, embedding_dim: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n        pe = torch.zeros(max_len, 1, embedding_dim)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Injects positional information into the input embeddings.\n\n        Arguments:\n            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(1)].transpose(0, 1)\n        return self.dropout(x)\n\n# Based on: https://github.com/Hanhpt23/Implement-Self-attention-Pytorch/blob/main/self-attention.py\nclass EncoderAttention(nn.Module):\n    \"\"\"    \n    Implementation of self-attention as described in 'Attention Is All You Need'.\n    \n    Note:\n    A sliding context window has been added to manage memory usage, which\n    can be high with long sequences.\n    \"\"\"\n    \n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.1):\n        '''dim: The embedding dimension of the input tokens.'''\n\n        super().__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divisible by num_heads {num_heads}.\"\n\n        self.context_window = 200\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # Using a single linear layer for Q, K, V is more efficient.\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, responses, padding_mask=None):\n        B, N, C = responses.shape\n\n        # self.qkv(x) results in [B, N, 3*C]\n        # .reshape -> [B, N, 3, h, C/h]\n        # .permute -> [3, B, h, N, C/h]\n        qkv = self.qkv(responses).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each becomes [B, h, N, C/h]\n\n        # Calculate scaled dot-product attention.\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n\n        seq_len = responses.size(1)\n    \n        # Create the sliding window mask.\n        # This masks positions where the distance between query and key is too large.\n        rows = torch.arange(seq_len, device=responses.device).unsqueeze(0)\n        cols = torch.arange(seq_len, device=responses.device).unsqueeze(1)\n        window_size = self.context_window // 2\n        \n        # 'window_mask' is True for positions outside the window, which will be masked.\n        window_mask = (rows - cols).abs() > window_size\n        \n        # Apply the sliding window mask.\n        attn = attn.masked_fill(window_mask.unsqueeze(0).unsqueeze(1), float('-inf'))\n\n        # Apply the padding mask.\n        if padding_mask is not None:\n            mask_reshaped = padding_mask.unsqueeze(1).unsqueeze(1)\n            attn = attn.masked_fill(mask_reshaped == 0, float('-inf'))\n\n        # Normalize with softmax over the key dimension.\n        attn = attn.softmax(dim=-1)\n        attn = torch.nan_to_num(attn)\n        attn = self.attn_drop(attn)\n\n        # Multiply with values and reshape for the final output.\n        # (attn @ v) -> [B, h, N, C/h]\n        # .transpose -> [B, N, h, C/h]\n        # .reshape -> [B, N, C]\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n# Similar to the encoder's attention, but with causal and sliding window masking.\nclass DecoderAttention(nn.Module):\n    \n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.1):\n\n        super().__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divisible by num_heads {num_heads}.\"\n\n        \n        self.dim = dim\n        self.context_window = 200\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # A single linear layer for Q, K, V is more efficient.\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, responses, padding_mask=None):\n        B, N, C = responses.shape\n\n        qkv = self.qkv(responses).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n\n        combined_mask = torch.zeros(N, N, device=q.device).bool()\n\n        # Causal mask to prevent attending to future tokens.\n        look_ahead_mask = torch.triu(torch.ones(N, N, device=responses.device), diagonal=1).bool()\n\n        # Sliding window mask.\n        rows = torch.arange(N, device=responses.device).unsqueeze(0)\n        cols = torch.arange(N, device=responses.device).unsqueeze(1)\n        window_size = self.context_window // 2\n        \n        # 'sliding_window_mask' is True for positions outside the window.\n        sliding_window_mask = (rows - cols).abs() > window_size\n\n        # Combine masks: a position is masked if it's in the future OR outside the window.\n        combined_mask = look_ahead_mask | sliding_window_mask\n            \n        # Apply the combined mask to the attention scores.\n        attn = attn.masked_fill(combined_mask.unsqueeze(0).unsqueeze(1), float('-inf'))\n\n        if padding_mask is not None:\n            mask_reshaped = padding_mask.unsqueeze(1).unsqueeze(1)\n            attn = attn.masked_fill(mask_reshaped == 0, float('-inf'))\n\n        attn = attn.softmax(dim=-1)\n        attn = torch.nan_to_num(attn)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n# Inspired by: https://gist.github.com/wolfecameron/5646b2092d41d6d31ec1abb28b3b930a\nclass CrossAttention(nn.Module):\n\n    def __init__(self, embedding_dim):\n        \"\"\"\n        Arguments:\n        embedding_dim: size of the embedding dimension.\n        \"\"\"\n        super().__init__()\n        self.d = embedding_dim\n        \n        # Linear projection for producing the query matrix.\n        self.w_q = nn.Linear(embedding_dim, embedding_dim, bias=False)\n        \n        # Linear projection for producing key and value matrices.\n        self.w_kv = nn.Linear(embedding_dim, 2*embedding_dim, bias=False)\n\n    def forward(self, x_1, x_2, padding_mask=None):\n        # Compute query, key, and value matrices.\n        q = self.w_q(x_1)\n        k, v = self.w_kv(x_2).split(self.d, dim=2)\n\n        # Compute the attention matrix.\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n\n        if padding_mask is not None:\n            # The mask corresponds to the keys/values (x_2).\n            mask_reshaped = padding_mask.unsqueeze(1).unsqueeze(1) # Reshape for broadcasting: [B, 1, 1, N_keys]\n            att = att.masked_fill(mask_reshaped == 0, float('-inf'))\n    \n        att = F.softmax(att, dim=-1)\n        att = torch.nan_to_num(att)\n    \n        y = att @ v\n        return y\n\nclass DifferentiableExplorerAttention(nn.Module):\n    def __init__(self, dim, num_heads=8, num_explorers=10, qkv_bias=False, proj_drop=0.1):\n        super().__init__()\n        assert dim % num_heads == 0, \"Embedding dim must be divisible by num_heads.\"\n        self.num_heads = num_heads\n        self.num_explorers = num_explorers\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        # 1. Standard QKV projection.\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        # 2. A small network to predict explorer positions.\n        #    It takes the query context and outputs a position for each explorer.\n        self.position_predictor = nn.Sequential(\n            nn.Linear(self.head_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, self.num_explorers) # Output E positions\n        )\n\n    def forward(self, x, padding_mask=None):\n        B, N, C = x.shape\n        H, E = self.num_heads, self.num_explorers\n\n        # --- Step 1: Standard QKV ---\n        qkv = self.qkv(x).reshape(B, N, 3, H, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each is [B, H, N, D]\n\n        # Base attention map shared by all explorers.\n        shared_attn_scores = (q @ k.transpose(-2, -1)) * self.scale # [B, H, N, N]\n\n        # --- Step 2: Predict Explorer Positions ---\n        # Use the mean of the queries as a context vector for position prediction.\n        context_q = q.mean(dim=2) # [B, H, D]\n        \n        # Predict positions, constrained between 0 and N-1.\n        predicted_positions = self.position_predictor(context_q).sigmoid() * (N - 1) # [B, H, E]\n\n        # --- Step 3: Create Soft \"Spotlight\" Masks from Positions ---\n        pos_indices = torch.arange(N, device=x.device, dtype=torch.float32).view(1, 1, 1, N) # [1, 1, 1, N]\n        centers = predicted_positions.unsqueeze(-1) # [B, H, E, 1]\n        \n        # A heuristic for the width of the spotlight.\n        spotlight_width = N / (E * 2) \n        \n        # Calculate Gaussian masks based on the predicted centers.\n        exponent = -((pos_indices - centers) ** 2) / (2 * spotlight_width ** 2)\n        spotlight_masks = torch.exp(exponent) # Shape: [B, H, E, N]\n\n        # --- Step 4: Apply Spotlights and Aggregate ---\n        # Modify the attention scores for each query based on where the explorers are looking.\n        # Reshape for broadcasting:\n        # shared_attn_scores: [B, H, N, N] -> [B, H, 1, N, N]\n        # spotlight_masks:    [B, H, E, N] -> [B, H, E, 1, N]\n        weighted_attn_scores = shared_attn_scores.unsqueeze(2) + spotlight_masks.unsqueeze(3)\n\n        if padding_mask is not None:\n            # The mask needs to be broadcastable to [B, H, E, N, N].\n            mask = padding_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n            weighted_attn_scores = weighted_attn_scores.masked_fill(mask == 0, float('-inf'))\n        \n        # Re-normalize with softmax for each explorer's view.\n        weighted_attn_probs = F.softmax(weighted_attn_scores, dim=-1) # [B, H, E, N, N]\n\n        # Get the output for all explorers and fuse them by averaging.\n        # v needs to be [B, H, 1, N, D] for broadcasting.\n        fused_output = (weighted_attn_probs @ v.unsqueeze(2)).mean(dim=2) # [B, H, N, D]\n\n        # --- Step 5: Final Projection ---\n        output = fused_output.transpose(1, 2).reshape(B, N, C)\n        output = self.proj(output)\n        output = self.proj_drop(output)\n\n        # --- Step 6: Repulsion Loss ---\n        # This encourages explorers to spread out and cover different parts of the sequence.\n        p = predicted_positions.view(B*H, E, 1)\n        p_diff = p.transpose(1, 2) - p # [B*H, E, E]\n        \n        # Use a Gaussian kernel for repulsion.\n        repulsion = torch.exp(-(p_diff ** 2) / (2 * spotlight_width ** 2))\n        \n        # Sum over pairs of different explorers, removing the diagonal (self-repulsion).\n        repulsion_loss = repulsion.sum(dim=(-1, -2)) - E\n        repulsion_loss = repulsion_loss.mean()\n\n        # The module returns the final output and the auxiliary loss term.\n        return output, repulsion_loss\n\nclass LayerNormalization(nn.Module):\n    \"\"\"\n    Implements Layer Normalization.\n\n    Normalizes the inputs across the features for each data sample,\n    making the computation independent of batch size.\n\n    y = (x - E[x]) / sqrt(Var[x] + eps) * gamma + beta\n    \"\"\"\n    def __init__(self, d_model: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        # 'gamma' is a learnable scale parameter.\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        # 'beta' is a learnable shift parameter.\n        self.beta = nn.Parameter(torch.zeros(d_model))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # --- The Core Logic of Layer Normalization ---\n\n        # 1. Calculate the mean and variance across the feature dimension.\n        #    'keepdim=True' is important for broadcasting.\n        mean = x.mean(dim=-1, keepdim=True)\n        # Using population variance, which is common in ML.\n        var = x.var(dim=-1, unbiased=False, keepdim=True)\n\n        # 2. Normalize the input tensor.\n        normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n\n        # 3. Scale and shift with the learnable parameters.\n        output = self.gamma * normalized_x + self.beta\n\n        return output\n\nclass FeedForward(nn.Module):\n    \"\"\"\n    The Position-wise Feed-Forward Network from 'Attention Is All You Need'.\n\n    Note:\n    Typically, d_ff is 4 times d_model, but I'm using a smaller size\n    to reduce computational intensity.\n    \"\"\"\n    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # (batch_size, sequence_length, d_model) -> (batch_size, sequence_length, d_ff)\n        x = self.linear_1(x)\n        x = self.relu(x)\n\n        # Applying dropout for regularization.\n        x = self.dropout(x)\n\n        # (batch_size, sequence_length, d_ff) -> (batch_size, sequence_length, d_model)\n        x = self.linear_2(x)\n\n        return x\n\n# In the EncoderLayer class\n\nclass EncoderLayer(nn.Module):\n    \"\"\"    \n    A single layer of the Encoder, based on 'Attention Is All You Need'.\n    \n    Note:\n    The normalization layer is placed before the attention and residual connections\n    (pre-norm) to help with gradient stability during training.\n    The commented-out prints are useful for debugging gradient issues.\n    \"\"\"\n    def __init__(self, embedding_dim):\n        super().__init__()\n        # Using the custom DifferentiableExplorerAttention.\n        self.input_attention = DifferentiableExplorerAttention(dim=embedding_dim, num_heads=8, num_explorers=10)\n        self.normalization1 = LayerNormalization(embedding_dim, 1e-6)\n        self.normalization2 = LayerNormalization(embedding_dim, 1e-6)\n        self.feed = FeedForward(embedding_dim, embedding_dim, 0.1)\n\n    def forward(self, pos_id_tensor, mask):\n        #print(f\"part_1_encoder_input (pos_id_tensor): shape={pos_id_tensor.shape}, mean={pos_id_tensor.mean():.4f}, std={pos_id_tensor.std():.4f}, has_inf={torch.isinf(pos_id_tensor).any()}, has_nan={torch.isnan(pos_id_tensor).any()}\")\n        normal_tensor = self.normalization1(pos_id_tensor)\n        #print(f\"part_2_encoder_norm_1: shape={normal_tensor.shape}, mean={normal_tensor.mean():.4f}, std={normal_tensor.std():.4f}, has_inf={torch.isinf(normal_tensor).any()}, has_nan={torch.isnan(normal_tensor).any()}\")\n        \n        # The attention module returns the output and the repulsion loss.\n        revised_tensor, repulsion_loss = checkp.checkpoint(self.input_attention, normal_tensor, mask, use_reentrant=True) \n        \n        #print(f\"part_3_encoder_input_attention: shape={revised_tensor.shape}, mean={revised_tensor.mean():.4f}, std={revised_tensor.std():.4f}, has_inf={torch.isinf(revised_tensor).any()}, has_nan={torch.isnan(revised_tensor).any()}\")\n        wise_tensor = pos_id_tensor + revised_tensor\n        #print(f\"part_4_encoder_add_1: shape={wise_tensor.shape}, mean={wise_tensor.mean():.4f}, std={wise_tensor.std():.4f}, has_inf={torch.isinf(wise_tensor).any()}, has_nan={torch.isnan(wise_tensor).any()}\")\n        \n        normal_tensor2 = self.normalization2(wise_tensor)\n        #print(f\"part_7_encoder_norm_2 (mistakenly labeled 7): shape={normal_tensor2.shape}, mean={normal_tensor2.mean():.4f}, std={normal_tensor2.std():.4f}, has_inf={torch.isinf(normal_tensor2).any()}, has_nan={torch.isnan(normal_tensor2).any()}\")\n        \n        ffn_output = self.feed(normal_tensor2)\n        #print(f\"part_5_encoder_ffn: shape={ffn_output.shape}, mean={ffn_output.mean():.4f}, std={ffn_output.std():.4f}, has_inf={torch.isinf(ffn_output).any()}, has_nan={torch.isnan(ffn_output).any()}\")\n        \n        wise_tensor2 = wise_tensor + ffn_output\n        #print(f\"part_6_encoder_add_2: shape={wise_tensor2.shape}, mean={wise_tensor2.mean():.4f}, std={wise_tensor2.std():.4f}, has_inf={torch.isinf(wise_tensor2).any()}, has_nan={torch.isnan(wise_tensor2).any()}\")\n        \n        # Pass the repulsion loss up to be handled by the main Encoder.\n        return wise_tensor2, repulsion_loss\n\nclass Encoder(nn.Module):\n    def __init__(self, embedding_dim, embedding, positional, num_layers):\n        super().__init__()\n        self.embedding = embedding\n        self.pos = positional\n        self.layers = nn.ModuleList([EncoderLayer(embedding_dim) for _ in range(num_layers)])\n        \n    def forward(self, tensor, mask):\n        id_tensor = self.embedding.forward(tensor)\n        pos_id_tensor = self.pos(id_tensor)\n\n        all_repulsion_losses = []\n\n        for layer in self.layers:\n            # Each layer returns the tensor and its repulsion loss.\n            pos_id_tensor, repulsion_loss = layer(pos_id_tensor, mask)\n            all_repulsion_losses.append(repulsion_loss)\n        \n        # Average the repulsion loss across all layers.\n        avg_repulsion_loss = torch.stack(all_repulsion_losses).mean()\n\n        # The encoder returns the final output and the average repulsion loss.\n        return pos_id_tensor, avg_repulsion_loss\n\nclass DecoderLayer(nn.Module):\n    \"\"\"    \n    A single layer of the Decoder, based on 'Attention Is All You Need'.\n    \n    Note:\n    Using pre-norm for gradient stability.\n    The commented-out prints are kept for debugging purposes.\n    \"\"\"\n    def __init__(self, embedding_dim):\n        super().__init__()\n        self.output_attention = DecoderAttention(embedding_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.1)\n        self.cross_attention = CrossAttention(embedding_dim)\n        self.normalization1 = LayerNormalization(embedding_dim, 1e-6)\n        self.normalization2 = LayerNormalization(embedding_dim, 1e-6)\n        self.normalization3 = LayerNormalization(embedding_dim, 1e-6)\n        self.feed = FeedForward(embedding_dim, embedding_dim, 0.1)\n        \n    def forward(self, encoder_tensor, padded_r, mask):\n        #print(f\"part_1_decoder_input (padded_r): shape={padded_r.shape}, mean={padded_r.mean():.4f}, std={padded_r.std():.4f}, has_inf={torch.isinf(padded_r).any()}, has_nan={torch.isnan(padded_r).any()}\")\n        normal_tensor = self.normalization1(padded_r)\n        #print(f\"part_2_decoder_norm_1: shape={normal_tensor.shape}, mean={normal_tensor.mean():.4f}, std={normal_tensor.std():.4f}, has_inf={torch.isinf(normal_tensor).any()}, has_nan={torch.isnan(normal_tensor).any()}\")\n        revised_tensor = checkp.checkpoint(self.output_attention, normal_tensor, mask, use_reentrant=True) # Batch, Sequence, Embedding\n        #print(f\"part_3_decoder_output_attention: shape={revised_tensor.shape}, mean={revised_tensor.mean():.4f}, std={revised_tensor.std():.4f}, has_inf={torch.isinf(revised_tensor).any()}, has_nan={torch.isnan(revised_tensor).any()}\")\n        wise_tensor = padded_r + revised_tensor # Batch, Sequence, Embedding\n        #print(f\"part_4_decoder_add_1: shape={wise_tensor.shape}, mean={wise_tensor.mean():.4f}, std={wise_tensor.std():.4f}, has_inf={torch.isinf(wise_tensor).any()}, has_nan={torch.isnan(wise_tensor).any()}\")\n        \n        normal_tensor2 = self.normalization2(wise_tensor)\n        #print(f\"part_5_decoder_norm_2: shape={normal_tensor2.shape}, mean={normal_tensor2.mean():.4f}, std={normal_tensor2.std():.4f}, has_inf={torch.isinf(normal_tensor2).any()}, has_nan={torch.isnan(normal_tensor2).any()}\")\n        revised_tensor = checkp.checkpoint(self.cross_attention, normal_tensor2, encoder_tensor, use_reentrant=True) # Batch, Sequence, Embedding\n        #print(f\"part_6_decoder_cross_attention: shape={revised_tensor.shape}, mean={revised_tensor.mean():.4f}, std={revised_tensor.std():.4f}, has_inf={torch.isinf(revised_tensor).any()}, has_nan={torch.isnan(revised_tensor).any()}\")\n        wise_tensor2 = wise_tensor + revised_tensor\n        #print(f\"part_7_decoder_add_2: shape={wise_tensor2.shape}, mean={wise_tensor2.mean():.4f}, std={wise_tensor2.std():.4f}, has_inf={torch.isinf(wise_tensor2).any()}, has_nan={torch.isnan(wise_tensor2).any()}\")\n\n        normal_tensor3 = self.normalization3(wise_tensor2)\n        #print(f\"part_8_decoder_output (normal_tensor3): shape={normal_tensor3.shape}, mean={normal_tensor3.mean():.4f}, std={normal_tensor3.std():.4f}, has_inf={torch.isinf(normal_tensor3).any()}, has_nan={torch.isnan(normal_tensor3).any()}\")\n        ffn_output = self.feed(normal_tensor3)\n        #print(f\"part_9_decoder_ffn: shape={ffn_output.shape}, mean={ffn_output.mean():.4f}, std={ffn_output.std():.4f}, has_inf={torch.isinf(ffn_output).any()}, has_nan={torch.isnan(ffn_output).any()}\")\n        wise_tensor3 = wise_tensor2 + ffn_output\n        #print(f\"part_10_decoder_add_3: shape={wise_tensor3.shape}, mean={wise_tensor3.mean():.4f}, std={wise_tensor3.std():.4f}, has_inf={torch.isinf(wise_tensor3).any()}, has_nan={torch.isnan(wise_tensor3).any()}\")\n\n        return wise_tensor3\n\nclass Decoder(nn.Module):\n    \"\"\"    \n    The Decoder module, composed of multiple DecoderLayers.\n    \"\"\"\n    def __init__(self, embedding_dim, embedding, positional, num_layers):\n        super().__init__()\n        self.embedding = embedding\n        self.pos = positional\n        self.layers = nn.ModuleList([DecoderLayer(embedding_dim) for _ in range(num_layers)])\n        \n    def forward(self, encoder_tensor, responses, mask_r):\n        id_tensor = self.embedding.forward(responses)\n        pos_id_tensor = self.pos(id_tensor) #Batch, Sequence, Embedding\n\n        for layer in self.layers:\n            pos_id_tensor = layer(encoder_tensor, pos_id_tensor, mask_r)\n\n        return pos_id_tensor\n\nclass Cleopatra(nn.Module):\n    def __init__(self, decoder, encoder, embedding_dim, total_num, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.ln_0 = nn.Linear(embedding_dim, total_num, bias=False)\n        self.device = device\n        # Hyperparameter for the \"Small-World\" repulsion margin.\n        self.sw_repulsion_margin = 0.5\n\n    def calculate_small_world_repulsion_loss(self, encoder_output):\n        \"\"\"\n        Calculates the \"Small-World\" repulsion loss on the encoder's output embeddings.\n        This encourages distinct concepts within the batch to have separate representations.\n        \"\"\"\n        # 1. Get a single vector representation for each sequence by mean pooling.\n        #    Shape: [batch_size, seq_len, embedding_dim] -> [batch_size, embedding_dim]\n        sentence_embeddings = encoder_output.mean(dim=1)\n        \n        batch_size = sentence_embeddings.size(0)\n        # The loss is only meaningful if there are pairs to compare.\n        if batch_size <= 1:\n            return torch.tensor(0.0, device=encoder_output.device)\n\n        # 2. Efficiently calculate all pairwise L2 distances.\n        pairwise_dist = torch.cdist(sentence_embeddings, sentence_embeddings, p=2)\n        \n        # 3. Identify \"blurry\" concepts by finding the \"hardest negative\".\n        #    To do this, we ignore the distance of each embedding to itself (the diagonal).\n        diagonal_mask = torch.eye(batch_size, device=pairwise_dist.device, dtype=torch.bool)\n        \n        # The hardest negative for each concept is the one closest to it.\n        hardest_negative_dist, _ = torch.min(pairwise_dist, dim=1)\n        \n        # 4. Apply the targeted repulsion loss.\n        #    The loss is max(0, margin - distance), penalizing only pairs that are too close.\n        loss = torch.clamp(self.sw_repulsion_margin - hardest_negative_dist, min=0)\n        \n        # Return the average loss across the batch.\n        return loss.mean()\n    \n    def forward(self, questions, responses, mask_r, mask_q):\n        # 1. Encoder runs (output variable renamed for clarity)\n        enc_output, explorer_repulsion_loss = self.encoder(questions, mask_q)\n        \n        # 2. **NEW STEP**: Calculate the second repulsion loss\n        sw_repulsion_loss = self.calculate_small_world_repulsion_loss(enc_output)\n        \n        # 3. Decoder runs (unchanged)\n        dec_output = self.decoder(enc_output, responses, mask_r)\n        logits = self.ln_0(dec_output)\n        \n        # 4. **NEW STEP**: Combine the two losses\n        total_repulsion_loss = explorer_repulsion_loss + sw_repulsion_loss\n        \n        # 5. The combined loss is returned\n        return logits, total_repulsion_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_and_process_batch(batch_pairs):\n    \"\"\"\n    Collates a batch of string pairs, converting them into padded\n    and masked tensors for model input.\n    \"\"\"\n    # Separate questions and responses.\n    questions, responses = zip(*batch_pairs)\n    \n    # Convert sentences to numerical indices.\n    questions_indexed = [indexesFromSentence(tokenizer, sentence, SOS_token, EOS_token) for sentence in questions]\n    responses_indexed = [indexesFromSentence(tokenizer, sentence, SOS_token, EOS_token) for sentence in responses]\n    \n    # Convert lists of indices to tensors.\n    question_tensors = [torch.LongTensor(q) for q in questions_indexed]\n    response_tensors = [torch.LongTensor(r) for r in responses_indexed]\n    \n    # Calculate lengths.\n    question_lengths = torch.tensor([len(q) for q in question_tensors], dtype=torch.long)\n    response_lengths = torch.tensor([len(r) for r in response_tensors], dtype=torch.long)\n\n    # Pad questions and responses.\n    padded_questions = pad_tensor(question_tensors, PAD_token)\n    padded_responses = pad_tensor(response_tensors, PAD_token)\n    \n    # Create masks.\n    mask_questions = mask_tensor(padded_questions, PAD_token)\n    mask_responses = mask_tensor(padded_responses, PAD_token)\n\n    return padded_questions, question_lengths, padded_responses, response_lengths, mask_questions, mask_responses\n\ndef save_checkpoint(directory, filename, model_state, optimizer_state, scheduler_state, scaler_state, stats):\n    \"\"\"Saves a comprehensive training checkpoint.\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    checkpoint_path = os.path.join(directory, filename)\n\n    # Create a single dictionary to hold all necessary information for resuming training.\n    checkpoint = {\n        'model_state_dict': model_state,\n        'optimizer_state_dict': optimizer_state,\n        'scheduler_state_dict': scheduler_state,\n        'scaler_state_dict': scaler_state,\n        # Unpack the dictionary of metrics and other info.\n        **stats \n    }\n    \n    torch.save(checkpoint, checkpoint_path)\n    print(f\"\\n--- Checkpoint Saved ---\")\n    print(f\"Path: {checkpoint_path}\")\n    print(f\"Epoch: {stats.get('epoch', 'N/A')}, Global Step: {stats.get('global_step', 'N/A')}\")\n    print(f\"Metrics: Total Loss={stats['metrics'].get('total_loss', 0):.4f}, Accuracy={stats['metrics'].get('accuracy', 0):.2f}%\")\n    print(f\"------------------------\\n\")\n\ndef training_goal_accumulate(cleopatra, criterion, padded_q, padded_r, mask_r, mask_q, device, temperature):\n    \"\"\"\n    This function now performs the forward pass and returns the main loss\n    and the auxiliary repulsion loss.\n    \"\"\"\n    decoder_input = padded_r[:, :-1]\n    mask_r = mask_r[:, :-1]\n    targets = padded_r[:, 1:].to(device)\n\n    with torch.amp.autocast('cuda'):\n        # The model returns logits and the repulsion loss\n        logits, repulsion_loss = cleopatra(padded_q, decoder_input, mask_r, mask_q)\n\n        if repulsion_loss.dim() > 0:\n            repulsion_loss = repulsion_loss.mean()\n\n        logits = logits / temperature\n        \n        # 1. Calculate the main cross-entropy loss\n        flat_logits = logits.view(-1, logits.shape[-1])\n        flat_targets = targets.reshape(-1)\n        main_loss = criterion(flat_logits, flat_targets)\n    \n    # Return the main loss, repulsion loss, and tensors for accuracy calculation\n    return main_loss, repulsion_loss, flat_logits, flat_targets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Constants ---\nMIN_COUNT = 1\nmax_length = 1000\ndropout = 0.1\nbatch = 10\nembedding_dim = 256\nlayers = 3\nepochs = 20\naccumulation_steps = 8\n\nTEACHER_FORCING_RATIO = 1.0\nTEACHER_FORCING_DECAY = 0.01\ncheckpoint = True\nCHECKPOINT_PATH = \"/kaggle/input/cleoprototype100k/pytorch/default/10/CleoPrototype_E1_S20000.pt\"\n\n\"\"\"\nKey Metrics for Training:\n\nLoss: The primary measure of how well the model is performing. A lower score is better.\n\nPerplexity: Measures how \"surprised\" the model is by the next token. It's derived from the cross-entropy loss. Lower is better.\n\nAccuracy: The percentage of times the model's top prediction is correct. More intuitive than loss.\n\nGradNorm: The overall size of the gradients. Helps diagnose exploding gradients (if the norm is very large) or vanishing gradients (if the norm is close to zero).\n\nLearningRate: Tracking the learning rate ensures the scheduler is working as intended.\n\"\"\"\n\ndef main_worker(epochs, data_loader, SOS_token, EOS_token, PAD_token, checkpoint, num_warmup_steps, num_training_steps):\n    embedding = Embedding(total_tokens, embedding_dim)\n    pos_encoding = PositionalEncoding(embedding_dim, dropout)\n    encoder = Encoder(embedding_dim, embedding, pos_encoding, layers)\n    decoder = Decoder(embedding_dim, embedding, pos_encoding, layers)\n    \n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    cleopatra = Cleopatra(decoder, encoder, embedding_dim, total_tokens, device)\n    cleopatra.ln_0.weight = embedding.embedding.weight\n\n    if checkpoint:\n        print(\"Loading checkpoint...\")\n        checkpoint_data = torch.load(CHECKPOINT_PATH, weights_only=False, map_location=device)\n        \n        # --- Load Model State ---\n        # Handle models saved with DataParallel, which adds a 'module.' prefix.\n        model_state_dict = checkpoint_data['model_state_dict']\n        if any(key.startswith('module.') for key in model_state_dict):\n            print(\"Model was trained with DataParallel. Removing 'module.' prefix from keys.\")\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in model_state_dict.items():\n                name = k[7:] # remove `module.`\n                new_state_dict[name] = v\n            model_state_dict = new_state_dict\n            \n        cleopatra.load_state_dict(model_state_dict)\n        print(\"Model loaded successfully!\")\n\n    cleopatra.to(device)\n\n    if torch.cuda.device_count() > 1:\n        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n        cleopatra = nn.DataParallel(cleopatra)\n\n    cleopatra = torch.compile(cleopatra)\n    \n    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n    optimizer = torch.optim.AdamW(cleopatra.parameters(), lr=0.0001)\n    \n    if checkpoint:\n        optimizer.load_state_dict(checkpoint_data['optimizer_state_dict'])\n    \n    scheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=num_training_steps\n    )\n    \n    print(f\"Total training steps: {num_training_steps}\")\n    print(f\"Warmup steps: {num_warmup_steps}\")\n\n    scaler = torch.amp.GradScaler('cuda')\n\n    INITIAL_TEMP = 4.0\n    FINAL_TEMP = 1.0\n\n    ANNEAL_START_STEP = 5000 \n    ANNEAL_END_STEP = 25000 \n\n    print(\"\\nStarting training...\")\n    for epoch in range(epochs):\n        for i, (padded_q, len_q, padded_r, len_r, mask_q, mask_r) in enumerate(data_loader):\n            \n            # --- Calculate the current temperature for this step ---\n            optimizer_step = (i + 1) // accumulation_steps # The actual step number.\n            if optimizer_step < ANNEAL_START_STEP:\n                current_temp = INITIAL_TEMP\n            elif optimizer_step >= ANNEAL_END_STEP:\n                current_temp = FINAL_TEMP\n            else:\n                # Linearly anneal the temperature.\n                progress = (optimizer_step - ANNEAL_START_STEP) / (ANNEAL_END_STEP - ANNEAL_START_STEP)\n                current_temp = INITIAL_TEMP - progress * (INITIAL_TEMP - FINAL_TEMP)\n            \n            # The function returns both the main and repulsion losses.\n            main_loss, repulsion_loss, flat_logits, flat_targets = training_goal_accumulate(\n            cleopatra, criterion, padded_q, padded_r, mask_r, mask_q, device, current_temp\n            )\n\n            # --- Combine the losses ---\n            # 'repulsion_weight' is a hyperparameter to balance the main task with the diversity objective.\n            repulsion_weight = 0.01 \n            total_loss = main_loss + repulsion_weight * repulsion_loss\n        \n            # Normalize the loss for gradient accumulation.\n            total_loss = total_loss / accumulation_steps\n            scaler.scale(total_loss).backward()\n        \n            # --- Backpropagation ---\n            if (i + 1) % accumulation_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                scheduler.step()\n        \n            # --- Logging ---\n            if i % 1 == 0:\n                \n                perplexity = torch.exp(main_loss).item()\n                preds = torch.argmax(flat_logits.detach(), dim=1)\n                non_pad = (flat_targets != PAD_token)\n                correct = (preds[non_pad] == flat_targets[non_pad]).sum().item()\n                total_non_pad = non_pad.sum().item()\n                accuracy = (correct / total_non_pad) * 100 if total_non_pad > 0 else 0.0\n                learning_rate = optimizer.param_groups[0]['lr']\n\n                # --- Analytical Logging ---\n                print(f\"Epoch {epoch+1} - Step {(i + 1) // accumulation_steps}/{len(data_loader) // accumulation_steps}\")\n                print(f\"  - Total Loss    : {total_loss.item():.4f}\")\n                print(f\"  - Main Loss     : {main_loss.item():.4f}\")\n                print(f\"  - Repulsion Loss: {repulsion_loss.item():.4f}\") \n                print(f\"  - Perplexity (PPL): {perplexity:.2f}\")\n                print(f\"  - Accuracy      : {accuracy:.2f}%\")\n                print(f\"  - Learning Rate : {learning_rate:.9f}\")\n                print(\"-\" * 20)\n\n                del preds, non_pad\n\n            if i != 0 and i % 20000 == 0:\n                global_step = (i + 1) // accumulation_steps\n                stats = {\n                    'epoch': epoch + 1,\n                    'global_step': global_step,\n                    'tokenizer': tokenizer, # Saving the tokenizer is good practice.\n                    'metrics': {\n                        'total_loss': total_loss.item() * accumulation_steps, # Un-normalize for logging.\n                        'main_loss': main_loss.item(),\n                        'repulsion_loss': repulsion_loss.item(),\n                        'perplexity': perplexity,\n                        'accuracy': accuracy,\n                        'learning_rate': learning_rate,\n                        'temperature': current_temp\n                    }\n                }\n\n                # Call the save checkpoint function.\n                save_checkpoint(\n                    directory=\"/kaggle/working/\", \n                    filename=f\"CleoPrototype_E{epoch+1}_S{global_step}.pt\", # A more descriptive filename.\n                    model_state=cleopatra.module.state_dict(), \n                    optimizer_state=optimizer.state_dict(), \n                    scheduler_state=scheduler.state_dict(),\n                    scaler_state=scaler.state_dict(),\n                    stats=stats\n                )\n        \n            del padded_q, padded_r, len_q, len_r, mask_q, mask_r, flat_logits, flat_targets\n            torch.cuda.empty_cache()\n\n\n# --- Main Execution ---\nif __name__ == '__main__':\n    \n    # --- 1. Load data and initialize tokenizer ---\n    file_path = \"/kaggle/input/generic/generic_data.json\"\n    token_path = \"/kaggle/input/sub-tokens/my_tokenizer.json\"\n    \n    # Initialize vocabulary.\n    print(\"Building vocabulary...\")\n    pairs, tokenizer, total_tokens = load_and_prepare_data(file_path, token_path)\n\n    SOS_token = tokenizer.token_to_id(\"[SOS]\")\n    EOS_token = tokenizer.token_to_id(\"[EOS]\")\n    PAD_token = tokenizer.token_to_id(\"[PAD]\")\n\n    chat_dataset = ChatDataset(pairs)\n\n    bucket_size = batch * 100\n\n    # Create the bucket sampler instance.\n    bucket_sampler = BucketRandomSampler(\n        data_source=chat_dataset,\n        bucket_size=bucket_size,\n        batch_size=batch\n    )\n\n    data_loader = torch.utils.data.DataLoader(\n        dataset=chat_dataset,\n        batch_size=batch,\n        sampler=bucket_sampler,\n        collate_fn=collate_and_process_batch,\n        # shuffle should not be set when using a custom sampler.\n    )\n\n    print(f\"\\nData loaded into DataLoader with {len(data_loader)} batches.\")\n    \n    num_training_steps = epochs*len(data_loader)\n    num_warmup_steps = 4000 \n\n    main_worker(epochs, data_loader, SOS_token, EOS_token, PAD_token, checkpoint, num_warmup_steps, num_training_steps)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}